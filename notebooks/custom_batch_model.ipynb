{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Paralellization with `CustomBatchModel` in Okareo\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-cookbook/blob/main/tutorials/notebooks/custom_batch_model.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "In this notebook, we show you how to use the `CustomBatchModel` class with parallelization to speed up your custom model evaluations in Okareo.\n",
    "\n",
    "To parallelize our custom model calls, we will run a Hugging Face model on a GPU-enabled machine. If your custom model uses an API endpoint and that endpoint supports parallelized batch inference, then `CustomBatchModel` can support your use case as well.\n",
    "\n",
    "## ðŸŽ¯ Goals\n",
    "\n",
    "After using this notebook, you will be able to:\n",
    "- Upload a scenario to Okareo\n",
    "- Define a pre-trained LLM as a `CustomBatchModel` in Okareo\n",
    "- Vary `batch_size` to find the fastest configuration for your Okareo evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload a scenario to Okareo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Okareo client\n",
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = \"<YOUR_OKAREO_API_KEY>\"\n",
    "okareo = Okareo(OKAREO_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# For making an ephemeral directory when downloading/uploading the scenario\n",
    "import tempfile\n",
    "\n",
    "webbizz_url = \"https://raw.githubusercontent.com/okareo-ai/okareo-python-sdk/main/examples/webbizz_classification_questions.jsonl\"\n",
    "webbizz_questions = os.popen(f\"curl {webbizz_url}\").read()\n",
    "temp_dir = tempfile.gettempdir()\n",
    "file_path = os.path.join(temp_dir, \"webbizz_classification_questions.jsonl\")\n",
    "with open(file_path, \"w+\") as file:\n",
    "    file.write(webbizz_questions)\n",
    "\n",
    "source_scenario = okareo.upload_scenario_set(file_path=file_path, scenario_name=\"Webbizz Questions Scenario\")\n",
    "\n",
    "print(source_scenario.app_link)\n",
    "\n",
    "# clean up tmp file\n",
    "os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Phi-3 from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.36s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_PREAMBLE = \"\"\"### Instruction:\n",
    "Given \"Input\", return a category under \"Output\" that is one of the following:\n",
    "\n",
    "- returns\n",
    "- pricing\n",
    "- complaints\n",
    "\n",
    "Return only one category that is most relevant to the question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `CustomBatchModel` for Phi-3\n",
    "\n",
    "In contrast with the `CustomModel` class, `CustomBatchModel` requires you to define an `invoke_batch` method, which takes an `input_batch` as an input. `input_batch` is a list of dictionaries taking the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_batch = [\n",
    "    {\n",
    "        \"id\": \"<UUID-FOR-SCENARIO-ROW-1>\",\n",
    "        \"input_value\": \"what is the cost of a Webbizz membership?\"\n",
    "    },\n",
    "    ...,\n",
    "    {\n",
    "        \"id\": \"<UUID-FOR-SCENARIO-ROW-N>\",\n",
    "        \"input_value\": \"how can I get help with a log in issue?\"\n",
    "    },\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an `input_batch`, `invoke_batch` returns a corresponding list of invocations with the following format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "return [\n",
    "    {\n",
    "        \"id\": \"<UUID-FOR-SCENARIO-ROW-1>\",\n",
    "        \"model_invocation\": ModelInvocation(\n",
    "            model_prediction=\"pricing\",\n",
    "            model_input=\"what is the cost of a Webbizz membership?\",\n",
    "            model_output_metadata={ ... },\n",
    "        )\n",
    "    },\n",
    "    ...,\n",
    "    {\n",
    "        \"id\": \"<UUID-FOR-SCENARIO-ROW-N>\",\n",
    "        \"input_value\": \n",
    "        \"model_invocation\": ModelInvocation(\n",
    "            model_prediction=\"complaints\",\n",
    "            model_input=\"how can I get help with a log in issue?\",\n",
    "            model_output_metadata={ ... },\n",
    "        )\n",
    "    },\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on the `invoke_batch` interface, you can read the docstring below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mCustomBatchModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_batch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mokareo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_under_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelInvocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "method for taking a batch of scenario inputs and returning a corresponding batch of model outputs\n",
      "\n",
      "arguments:\n",
      "-> input_batch: list[dict[str, Union[dict, list, str]]] - batch of inputs to the model. Expects a list of\n",
      "dicts of the format { 'id': str, 'input_value': Union[dict, list, str] }.\n",
      "\n",
      "returns:\n",
      "-> list of dicts of format { 'id': str, 'model_invocation': Union[ModelInvocation, Any] }. 'id' must match\n",
      "the corresponding input_batch element's 'id'.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/phi3/lib/python3.11/site-packages/okareo/model_under_test.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "?CustomBatchModel.invoke_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up a `CustomBatchModel` to use our Hugging Face model in its `invoke_batch` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo.model_under_test import CustomBatchModel, ModelInvocation\n",
    "\n",
    "def format_instruction(sample):\n",
    "\tprompt = f\"\"\"{PROMPT_PREAMBLE}\n",
    "### Input:\n",
    "{sample}\n",
    " \n",
    "### Output:\n",
    "\"\"\"\n",
    "\treturn prompt\n",
    "\n",
    "mut_name = f\"WebBizz Intent Detection - Phi-3-mini-4k (unquantized, zero-shot)\"\n",
    "\n",
    "class Phi3ModelUnquantized(CustomBatchModel):\n",
    "    def __init__(self, name, batch_size):\n",
    "        super().__init__(name, batch_size)\n",
    "        self.categories = [\n",
    "            \"returns\",\n",
    "            \"pricing\",\n",
    "            \"complaints\",\n",
    "        ]\n",
    "\n",
    "    def invoke_batch(self, input_batch):\n",
    "        # unpack the input_values, ids from the batch\n",
    "        input_values = [input_dict['input_value'] for input_dict in input_batch]\n",
    "        scenario_ids = [input_dict['id'] for input_dict in input_batch]\n",
    "\n",
    "        # format the inputs as instructions\n",
    "        prompts = [format_instruction(value) for value in input_values]\n",
    "\n",
    "        # perform batch inference using the HF model\n",
    "        input_ids = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).input_ids.cuda()\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=2, # change based on expected len of generations\n",
    "            temperature=0.0,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        decoded_batch = tokenizer.batch_decode(\n",
    "             outputs.detach().cpu().numpy(), skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # pack the decoded generations in a list of dicts with id + ModelInvocation\n",
    "        invocations = []\n",
    "        for scenario_id, decoded, prompt, input_value in zip(scenario_ids, decoded_batch, prompts, input_values):\n",
    "            pred = decoded[len(prompt):].strip() # only use generation past the instruction prompt\n",
    "            res = \"unknown\"\n",
    "            for cat in self.categories:\n",
    "                if cat in pred:\n",
    "                    res = cat\n",
    "            invocations.append({\n",
    "                'id': scenario_id,\n",
    "                'model_invocation': ModelInvocation(\n",
    "                    model_prediction=res,\n",
    "                    model_input=input_value,\n",
    "                    model_output_metadata=pred,\n",
    "                )\n",
    "            })\n",
    "        return invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run `CustomBatchModel` with different batch sizes\n",
    "\n",
    "Run the same evaluation with different values of `batch_size` to find the fastest configuration for our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- evaluation on train split ---\n",
      "batch_size | eval_time (s) | app_link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 22.89 | https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/c8e52165-1d55-4560-a5c9-f94abe1a37f7\n",
      "2 | 16.70 | https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/0eed5954-c659-401a-bf86-d9b015599052\n",
      "4 | 14.09 | https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/d8e25bd1-1f09-4133-9d41-df32e241e736\n",
      "8 | 12.25 | https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/1015fa14-3271-4595-bd0d-7eb79d84cc01\n",
      "16 | 11.57 | https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/14c8d065-0cb7-4713-8b96-663a8b60e8de\n",
      "32 | 11.28 | https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/48c835f6-ff6a-4d0c-9acb-15d54c32bae7\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from okareo_api_client.models.test_run_type import TestRunType\n",
    "\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "\n",
    "print(f'--- evaluation on train split ---')\n",
    "print(f'batch_size | eval_time (s) | app_link')\n",
    "for batch_size in batch_sizes:\n",
    "    # Register the model to use in the test run\n",
    "    start_time = time()\n",
    "    model_under_test = okareo.register_model(\n",
    "        name=mut_name,\n",
    "        model=[\n",
    "            Phi3ModelUnquantized(\n",
    "                name=Phi3ModelUnquantized.__name__,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "        ],\n",
    "        update=True\n",
    "    )\n",
    "\n",
    "    eval_name = f\"Intent Detection (batch_size={batch_size})\"\n",
    "    evaluation = model_under_test.run_test(\n",
    "        name=eval_name,\n",
    "        scenario=source_scenario.scenario_id,\n",
    "        test_run_type=TestRunType.MULTI_CLASS_CLASSIFICATION,\n",
    "        calculate_metrics=True,\n",
    "    )\n",
    "    eval_time = time() - start_time\n",
    "    print(f\"{batch_size} | {eval_time:3.2f} | {evaluation.app_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On our GPU, the fastest evaluation time is achieved at `batch_size=32`, which takes less than half the time of `batch_size=1`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
