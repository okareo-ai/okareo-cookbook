{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/notebooks/intent_class_synthetic/training_synthetic.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "\n",
    "\n",
    "## Training the Synthetic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for data processing\n",
    "import pandas as pd\n",
    "\n",
    "# Read all of the synthetic data files\n",
    "files = [\"cond.csv\", \"contr.csv\", \"rephrased.csv\", \"spelling.csv\", \"training.csv\"]\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(\"data/\" + file)\n",
    "    dfs.append(df)\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Convert the \"result\" column to numeric classes\n",
    "data[\"label\"] = data[\"result\"].map({\"complaints\": 2, \"returns\": 1, \"pricing\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use DistilBERT as the backbone of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a DistilBERT model and tokenizer\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\", num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly split our training data and create dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries for PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = data.sample(frac=0.8)\n",
    "val_data = data.drop(train_data.index)\n",
    "\n",
    "# Create a custom dataset class for the text data\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the text and label for the given index\n",
    "        text = self.data.iloc[idx][\"input\"]\n",
    "        label = self.data.iloc[idx][\"label\"]\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "        # Return the input IDs, attention mask, and label (reshape the input IDs and attention mask to remove an unneeded dimension)\n",
    "        return encoding.input_ids.squeeze(), encoding.attention_mask.squeeze(), label\n",
    "\n",
    "# Create Dataset objects for the training and validation sets\n",
    "train_dataset = TextDataset(train_data, tokenizer)\n",
    "val_dataset = TextDataset(val_data, tokenizer)\n",
    "\n",
    "# Create DataLoader objects for the training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and validation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# An optimizer for the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# One epoch of training\n",
    "def train(loader, model, optimizer):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    print(\"Training...\")\n",
    "    losses = []\n",
    "    for input_ids, attention_mask, labels in tqdm(loader):\n",
    "        optimizer.zero_grad()\n",
    "        # One-hot encode the labels\n",
    "        oh_labels = torch.nn.functional.one_hot(labels, num_classes=3).to(torch.float32)\n",
    "        # Pass the input IDs, attention mask, and one-hot labels to the model and get the loss\n",
    "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=oh_labels).loss\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        # Track losses\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "# One epoch of validation\n",
    "def validate(loader, model, epoch):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    print(\"Validating...\")\n",
    "    # Disable gradient calculations (not needed for validation)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        losses = []\n",
    "        for input_ids, attention_mask, labels in loader:\n",
    "            # One-hot encode the labels\n",
    "            oh_labels = torch.nn.functional.one_hot(labels, num_classes=3).to(torch.float32)\n",
    "            # Pass the input IDs and attention mask to the model and get the logits and loss\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask, labels=oh_labels)\n",
    "            logits = output.logits\n",
    "            loss = output.loss\n",
    "            # Get the predicted labels\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            # Track accuracy\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(labels)\n",
    "            # Track losses\n",
    "            losses.append(loss.item())\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch}, Validation Accuracy: {accuracy}\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for 2 epochs. Since we're using much more data now, we don't need as many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the model for 2 epochs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(2):\n",
    "    # Train the model for one epoch\n",
    "    losses = train(train_loader, model, optimizer)\n",
    "    train_losses.extend(losses)\n",
    "    # Validate the model\n",
    "    val_loss = validate(val_loader, model, epoch)\n",
    "    val_losses.extend(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the per-batch training and validation. Since the number of batches is different between training and validation, this plot will look different than the plot for the Base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "plt.plot(losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Average Loss per Batch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model locally\n",
    "model.save_pretrained(\"blog_model_synthetic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
