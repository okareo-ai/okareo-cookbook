{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Fine-tuned Retrieval Models in Okareo\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/retrieval_embedding_finetuning_eval.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## ðŸŽ¯ Goals\n",
    "\n",
    "After using this notebook, you will be able to:\n",
    "- Evaluate a pre-trained embedding model in Okareo\n",
    "- Filter the results of the retrieval evaluation\n",
    "- Generate fine-tuning data based on the filtered results\n",
    "- Fine-tune the model with the generated data\n",
    "- Compare the performance of the embedding models pre/post fine-tuning in Okareo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement: Retrieval Model\n",
    "\n",
    "Suppose we are developing a RAG system that answers user questions.\n",
    "\n",
    "This notebook focuses on finetuning an open source embedding model, [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2), for the Context Retrieval component of the RAG pipeline.\n",
    "\n",
    "The purpose of Context Retrieval is to fetch relevant documents/chunks to build the context for a downstream generative model. The better performance we achieve on Retrieval, the higher quality the RAG's final output will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the pre-trained model in Okareo\n",
    "\n",
    "Here, we perform a retrieval evaluation in Okareo by:\n",
    "1. Uploading the retrieval data as a scenario in Okareo\n",
    "2. Defining a CustomModel for retrieval using ChromaDB to store our model's embeddings\n",
    "3. Run a retrieval evaluation on our CustomModel with the scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = os.environ[\"OKAREO_API_KEY\"]\n",
    "okareo = Okareo(OKAREO_API_KEY)\n",
    "random_string = ''.join(random.choices(string.ascii_letters, k=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo_api_client.models import SeedData\n",
    "from okareo_api_client.models.scenario_set_create import ScenarioSetCreate\n",
    "\n",
    "import json\n",
    "\n",
    "scenarios = {}\n",
    "# LOAD BOTH TRAIN AND TEST DATA SCENARIOS\n",
    "for file_info in [\n",
    "    (\"./data/squad_qa_train_210.jsonl\", \"train\"), \n",
    "    (\"./data/squad_qa_test_90.jsonl\", \"test\")\n",
    "]:\n",
    "    filename, split = file_info\n",
    "    scenario_data = []\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            scenario_data.append(\n",
    "                SeedData(\n",
    "                    input_=data[\"input\"],\n",
    "                    result=[data[\"passage_id\"]]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Create scenario\n",
    "    request = ScenarioSetCreate(\n",
    "        name=f\"Autogen Retrieval - {split} - {random_string}\",\n",
    "        seed_data=scenario_data\n",
    "    )\n",
    "    scenario = okareo.create_scenario_set(request)\n",
    "    scenarios[split] = scenario\n",
    "    print(f\"{split}: {scenario.app_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload a custom retrieval model using a Chroma collection of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# CREATE INSTANCE OF CHROMADB AND LOAD CORPUS\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from chromadb.utils import embedding_functions\n",
    "from okareo.model_under_test import CustomModel, ModelInvocation\n",
    "\n",
    "\n",
    "corpus_file = \"./data/squad_corpus.jsonl\"\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "#embedding_model_name = \"/Users/mrpositive/Downloads/ft-models/autogen-retrieval-finetune\"\n",
    "\n",
    "collection_name = \"squad-corpus\"\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=embedding_model_name)\n",
    "\n",
    "\n",
    "def load_and_initialize_collection(collection_name: str, embedding_function: callable, corpus_file=\"./data/squad_corpus.jsonl\"):\n",
    "    # Load corpus from file\n",
    "    jsonObj = pd.read_json(path_or_buf=corpus_file, lines=True)\n",
    "    corpus = dict(zip(jsonObj.result, jsonObj.input))\n",
    "\n",
    "    chroma_client = chromadb.Client(chromadb.config.Settings(allow_reset=True))\n",
    "    chroma_client.reset() # chromadb has some weird'a bug that hangs on to some state\n",
    "\n",
    "    collection = chroma_client.get_or_create_collection(collection_name,\n",
    "                                                metadata={\"hnsw:space\": \"cosine\"},\n",
    "                                                embedding_function=embedding_function)\n",
    "\n",
    "    # Add the documents to the collection with the corresponding metadata\n",
    "    collection.add(\n",
    "        documents=list(jsonObj.input),\n",
    "        ids=[str(x) for x in list(jsonObj.result)],\n",
    "    )\n",
    "    \n",
    "    return collection, corpus\n",
    "\n",
    "collection, corpus = load_and_initialize_collection(collection_name, embedding_function)\n",
    "\n",
    "# A funtion to convert the query results from our ChromaDB collection into a list of dictionaries with the document ID, score, metadata, and label\n",
    "def query_results_to_score(results):\n",
    "    parsed_ids_with_scores = []\n",
    "    for i in range(0, len(results['distances'][0])):\n",
    "        # Create a score based on cosine similarity\n",
    "        score = (2 - results['distances'][0][i]) / 2\n",
    "        parsed_ids_with_scores.append(\n",
    "            {\n",
    "                \"id\": results['ids'][0][i],\n",
    "                \"score\": score,\n",
    "                \"metadata\": {\"context\": corpus[results['ids'][0][i]]},\n",
    "                \"label\": f\"Context w/ ID: {results['ids'][0][i]}\"\n",
    "            }\n",
    "        )\n",
    "    return parsed_ids_with_scores\n",
    "\n",
    "\n",
    "mut_name = f\"Retrieval Model - {embedding_model_name}\"\n",
    "\n",
    "class RetrievalModel(CustomModel):\n",
    "    def invoke(self, input: str):\n",
    "        results = collection.query(\n",
    "            query_texts=[input],\n",
    "            n_results=5\n",
    "        )\n",
    "        # return a tuple of (parsed_ids_with_scores, overall model response context)\n",
    "        return ModelInvocation(\n",
    "            model_prediction=query_results_to_score(results),\n",
    "            model_input=input,\n",
    "            model_output_metadata={'model_data': input}\n",
    "        )\n",
    "\n",
    "# Register the model to use in the test run\n",
    "model_under_test = okareo.register_model(\n",
    "    name=mut_name,\n",
    "    model=[RetrievalModel(name=RetrievalModel.__name__)],\n",
    "    update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See test eval results in Okareo: http://localhost:3000/project/47895342-8441-426b-bc86-e8dd831d2971/eval/79598552-4731-4230-bb8c-06443ac5667f\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE THE RETRIEVAL MODEL\n",
    "from okareo_api_client.models import TestRunType\n",
    "\n",
    "# Import the datetime module for timestamping\n",
    "from datetime import datetime\n",
    "\n",
    "# Define thresholds for the evaluation metrics\n",
    "at_k_intervals = [1, 2, 3, 4, 5] \n",
    "metrics_kwargs={\n",
    "    \"accuracy_at_k\": at_k_intervals ,\n",
    "    \"precision_recall_at_k\": at_k_intervals ,\n",
    "    \"ndcg_at_k\": at_k_intervals,\n",
    "    \"mrr_at_k\": at_k_intervals,\n",
    "    \"map_at_k\": at_k_intervals,\n",
    "}\n",
    "\n",
    "# Perform a test run using the uploaded scenario set\n",
    "test_runs = {}\n",
    "for split_name, seed_scenario in scenarios.items():\n",
    "    if split_name == \"train\":\n",
    "        continue\n",
    "    test_run_item = model_under_test.run_test(\n",
    "        scenario=seed_scenario, # use the scenario from the scenario set uploaded earlier\n",
    "        name=f\"Retrieval ({split_name}) - {datetime.now().strftime('%m-%d %H:%M:%S')}\", # add a timestamp to the test run name\n",
    "        test_run_type=TestRunType.INFORMATION_RETRIEVAL, # specify that we are running an information retrieval test\n",
    "        calculate_metrics=True,\n",
    "        # Define the evaluation metrics to calculate\n",
    "        metrics_kwargs=metrics_kwargs\n",
    "    )\n",
    "\n",
    "    # Generate a link back to Okareo for evaluation visualization\n",
    "    model_results = test_run_item.model_metrics.to_dict()\n",
    "    test_runs[split_name] = model_results\n",
    "    app_link = test_run_item.app_link\n",
    "    print(f\"See {split_name} eval results in Okareo: {app_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand the finetuning set with Failure Rows\n",
    "\n",
    "To improve our finetuned model, we need a fine-tuning set that is similar to our data. To do this, we extract rows from our retrieval evaluation based on some failure criteria, and we generate new queries based on these failed rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 37 queries have recall@3 <= 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.okareo.com/project/394c2c12-be7a-47a6-911b-d6c673bc543b/scenario/8946d33d-cd76-4a74-a2b0-aff48ce47f7e'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter IDs based on failure criteria\n",
    "K = \"3\"\n",
    "filter_thresh = 0.5\n",
    "filter_metric = \"recall\"\n",
    "\n",
    "# get failure rows from train split eval\n",
    "failed_ids = []\n",
    "for id, metrics in test_runs['train']['row_level_metrics'].items():\n",
    "    if metrics[K][filter_metric] <= filter_thresh:\n",
    "        failed_ids.append(id)\n",
    "print(f\"-> {len(failed_ids)} queries have {filter_metric}@{K} <= {filter_thresh}\")\n",
    "\n",
    "sdp = okareo.get_scenario_data_points(scenarios['train'].scenario_id)\n",
    "\n",
    "scenario_points = [\n",
    "    SeedData(input_=dp.input_, result=dp.result)\n",
    "    for dp in sdp\n",
    "    if dp.id in failed_ids\n",
    "]\n",
    "\n",
    "# create the scenario set for evaluation\n",
    "create_request = ScenarioSetCreate(\n",
    "    name=f\"Autogen Retrieval - train - failure rows - {random_string}\",\n",
    "    seed_data = scenario_points,\n",
    ")\n",
    "\n",
    "failure_scenario = okareo.create_scenario_set(create_request)\n",
    "failure_scenario.app_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new queries based on the failure rows\n",
    "from okareo_api_client.models.scenario_set_generate import ScenarioSetGenerate\n",
    "from okareo_api_client.models.generation_tone import GenerationTone\n",
    "from okareo_api_client.models.scenario_type import ScenarioType\n",
    "\n",
    "generate_request = ScenarioSetGenerate(\n",
    "    source_scenario_id=failure_scenario.scenario_id,\n",
    "    name=f\"Autogen Retrieval - train - augmented failure rows - {random_string}\",\n",
    "    number_examples=3,\n",
    "    generation_type=ScenarioType.REPHRASE_INVARIANT,\n",
    "    generation_tone=GenerationTone.NEUTRAL,\n",
    ")\n",
    "\n",
    "rephrased_scenario = okareo.generate_scenario_set(generate_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# format the scenario data points properly for the huggingface trainer\n",
    "# this requires that we pivot each {'query': ..., 'answers': [1, ..., N]}\n",
    "# to {'query': ..., 'answer': 1}, ..., {'query': ..., 'answer': N}\n",
    "rephrased_sdp = okareo.get_scenario_data_points('b4b0fff6-4e80-41fa-b6dd-f6c96a1a13b2')\n",
    "\n",
    "sdp = okareo.get_scenario_data_points(scenarios['train'].scenario_id)\n",
    "\n",
    "# augment the train data with the rephrased failure rows\n",
    "# use the augmented train set for finetuning\n",
    "finetuning_embedding_data = []\n",
    "for dp in sdp + rephrased_sdp:\n",
    "    for did in dp.result:\n",
    "        finetuning_embedding_data.append({'query': dp.input_, 'answer': corpus[did]})\n",
    "\n",
    "print(len(finetuning_embedding_data))\n",
    "\n",
    "\n",
    "file_path = f\"autogen_retrieval_finetuning_embedding_data.jsonl\"\n",
    "\n",
    "# write the finetuning data to a jsonl file\n",
    "with open(file_path, \"w\") as file:\n",
    "    for row in finetuning_embedding_data:\n",
    "        file.write(json.dumps(row) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the Model on the Augmented Train Split\n",
    "\n",
    "Use the train split and the rephrased failures to repeat fine-tune the embedding model, and then compare the retrieval performance of the fine-tuned model to the pretrained model in Okareo.\n",
    "\n",
    "For more details on fine-tuning embedding models, see this [huggingface blog on training sentence transformers](https://huggingface.co/blog/train-sentence-transformers#local-data-that-requires-pre-processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "\n",
    "# Load a model to train/finetune\n",
    "huggingface_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(huggingface_model_name)\n",
    "\n",
    "# Initialize the MultipleNegativesRankingLoss\n",
    "# This loss requires pairs of queries and related document chunks\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Load an example training dataset that works with our loss function:\n",
    "dataset = load_dataset(\"json\", data_files=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    "output_dir = f\"models/{huggingface_model_name}/autogen-retrieval-finetune\"\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=output_dir,\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=32, # reduced from 32\n",
    "    per_device_eval_batch_size=32, # reduced from 32\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,  # Set to False if your GPU can't handle FP16\n",
    "    bf16=False,  # Set to True if your GPU supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # Losses using \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-6, # added for smaller dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='212' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [212/212 00:17, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.478200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.288500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d69fd01cfb4e018ae0ee5c98250ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=212, training_loss=0.38013167313809665, metrics={'train_runtime': 17.1361, 'train_samples_per_second': 392.154, 'train_steps_per_second': 12.372, 'total_flos': 0.0, 'train_loss': 0.38013167313809665, 'epoch': 3.7735849056603774})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Save the trained model\n",
    "model.save(output_dir)\n",
    "\n",
    "# Load the saved model\n",
    "ft_model = SentenceTransformer(\"models/sentence-transformers/all-MiniLM-L6-v2/autogen-retrieval-finetune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the fine-tuned model\n",
    "\n",
    "Create a new ChromaDB collection with the updated document embeddings to use with our new fine-tuned CustomModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "\n",
    "chroma_client = chromadb.Client(chromadb.config.Settings(allow_reset=True))\n",
    "chroma_client.reset() # chromadb has some weird'a bug that hangs on to some state\n",
    "\n",
    "collection_name = \"retrieval_finetune_test\"\n",
    "# if collection_name in [col.name for col in chroma_client.list_collections()]:\n",
    "#     chroma_client.delete_collection(collection_name)\n",
    "finetuned_collection = chroma_client.create_collection(name=collection_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "corpus_file = \"./data/squad_corpus.jsonl\"\n",
    "jsonObj = pd.read_json(path_or_buf=corpus_file, lines=True)\n",
    "\n",
    "finetuned_collection.add(\n",
    "    documents=list(jsonObj.input),\n",
    "    ids=[str(x) for x in list(jsonObj.result)],\n",
    "    embeddings=ft_model.encode(list(jsonObj.input)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo.model_under_test import CustomModel, ModelInvocation\n",
    "\n",
    "\n",
    "mut_name = f\"Finetuned Retrieval Model - MiniLM-L6-v2\"\n",
    "\n",
    "class FinetunedRetrievalModel(CustomModel):\n",
    "    def invoke(self, input: str):\n",
    "        embeddings = ft_model.encode([input])\n",
    "        results = finetuned_collection.query(\n",
    "            query_embeddings=embeddings,\n",
    "            n_results=5\n",
    "        )\n",
    "        # return a tuple of (parsed_ids_with_scores, overall model response context)\n",
    "        return ModelInvocation(\n",
    "            model_prediction=query_results_to_score(results),\n",
    "            model_input=input,\n",
    "            model_output_metadata={'model_data': input}\n",
    "        )\n",
    "\n",
    "# Register the model to use in the test run\n",
    "ft_model_under_test = okareo.register_model(\n",
    "    name=mut_name,\n",
    "    model=[FinetunedRetrievalModel(name=FinetunedRetrievalModel.__name__)],\n",
    "    update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo_api_client.models import TestRunType\n",
    "\n",
    "# Import the datetime module for timestamping\n",
    "from datetime import datetime\n",
    "\n",
    "# Define thresholds for the evaluation metrics\n",
    "at_k_intervals = [1, 2, 3, 4, 5] \n",
    "metrics_kwargs={\n",
    "    \"accuracy_at_k\": at_k_intervals ,\n",
    "    \"precision_recall_at_k\": at_k_intervals ,\n",
    "    \"ndcg_at_k\": at_k_intervals,\n",
    "    \"mrr_at_k\": at_k_intervals,\n",
    "    \"map_at_k\": at_k_intervals,\n",
    "}\n",
    "\n",
    "# Perform a test run using the uploaded scenario set\n",
    "finetuned_test_runs = {}\n",
    "for split_name, seed_scenario in scenarios.items():\n",
    "    test_run_item = ft_model_under_test.run_test(\n",
    "        scenario=seed_scenario, # use the scenario from the scenario set uploaded earlier\n",
    "        name=f\"Retrieval ({split_name}) - {datetime.now().strftime('%m-%d %H:%M:%S')}\", # add a timestamp to the test run name\n",
    "        test_run_type=TestRunType.INFORMATION_RETRIEVAL, # specify that we are running an information retrieval test\n",
    "        calculate_metrics=True,\n",
    "        # Define the evaluation metrics to calculate\n",
    "        metrics_kwargs=metrics_kwargs\n",
    "    )\n",
    "\n",
    "    # Generate a link back to Okareo for evaluation visualization\n",
    "    finetuned_model_results = test_run_item.model_metrics.to_dict()\n",
    "    finetuned_test_runs[split_name] = finetuned_model_results\n",
    "    app_link = test_run_item.app_link\n",
    "    print(f\"See {split_name} eval results in Okareo: {app_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the results pre/post fine-tuning\n",
    "\n",
    "print(f\"Pre Fine-tuning | Post Fine-tuning | Difference\")\n",
    "for split_name in [\"train\", \"test\"]:\n",
    "    print(f\"------ {split_name} ------\")\n",
    "    model_results = test_runs[split_name]\n",
    "    finetuned_model_results = finetuned_test_runs[split_name]\n",
    "    for key in finetuned_model_results.keys():\n",
    "        if key == \"row_level_metrics\":\n",
    "            continue\n",
    "        print(f\"------ {key} ------\")\n",
    "        for K in at_k_intervals:\n",
    "            pre = model_results[key][str(K)]\n",
    "            post = finetuned_model_results[key][str(K)]\n",
    "            diff = post - pre\n",
    "            print(f\"K={K}: {pre:4.3f} | {post:4.3f} | {'+' if diff >= 0 else ''}{diff:4.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
