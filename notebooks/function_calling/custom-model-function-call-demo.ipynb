{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model Function Calling Evaluation Demo\n",
    "\n",
    "**ðŸŽ¯ Goal**:\n",
    "- Run a function calling evaluation in Okareo.\n",
    "- Provide a simple introduction to Okareo evaluations.\n",
    "\n",
    "**ðŸ“‹ Steps**:\n",
    "1. Upload a function calling scenario.\n",
    "2. Define a custom model to generate the function calls\n",
    "3. Run the evaluation using the scenario (from #1) + model (from #2) along with checks to measure function call accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install okareo\n",
    "%pip install okareo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Okareo client\n",
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = \"<YOUR_OKAREO_API_KEY>\"\n",
    "okareo = Okareo(OKAREO_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload a simple scenario. Each row of the `seed_data` should contain:\n",
    "\n",
    "- `input_`: query to the agent.\n",
    "- `result`: the expected function call that answers the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "from okareo_api_client.models.scenario_set_create import ScenarioSetCreate\n",
    "from okareo_api_client.models.seed_data import SeedData\n",
    "\n",
    "def random_string(length: int) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_letters, k=length))\n",
    "\n",
    "seed_data = [\n",
    "    SeedData(\n",
    "        input_=\"can you delete my account? my name is Bob\",\n",
    "        result={\"name\": \"delete_account\", \"parameter_definitions\": {\"username\": {\"value\": \"Bob\", \"type\": \"str\", \"required\": True}}},\n",
    "    ),\n",
    "    SeedData(\n",
    "        input_=\"how do I make an account? I'm Alice\",\n",
    "        result={\"name\": \"create_account\", \"parameter_definitions\": {\"username\": {\"value\": \"Alice\", \"type\": \"str\", \"required\": True}}},\n",
    "    ),\n",
    "    SeedData(\n",
    "        input_=\"how do I create an account?\",\n",
    "        result={\"name\": \"create_account\", \"parameter_definitions\": {\"username\": {\"value\": \"Alice\", \"type\": \"str\", \"required\": True}}},\n",
    "    ),\n",
    "    SeedData(\n",
    "        input_=\"my name is John. how do I create a project?\",\n",
    "        result={\"name\": \"create_account\", \"parameter_definitions\": {\"username\": {\"value\": \"Alice\", \"type\": \"str\", \"required\": True}}},\n",
    "    ),\n",
    "]\n",
    "\n",
    "tool_scenario = okareo.create_scenario_set(\n",
    "    ScenarioSetCreate(\n",
    "        name=f\"Function Call Demo Scenario - {random_string(5)}\",\n",
    "        seed_data=seed_data,\n",
    "    ) \n",
    ")\n",
    "\n",
    "print(tool_scenario.app_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the [CustomModel](https://docs.okareo.ai/docs/sdk/okareo_python#custommodel--modelinvocation) using conditionals, and register the model with Okareo.\n",
    "\n",
    "In reality, you would use the `CustomModel` class to invoke and parse your LLM's outputs. Feel free to play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo.model_under_test import CustomModel, ModelInvocation\n",
    "\n",
    "class FunctionCallModel(CustomModel):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        self.usernames = [\"Bob\", \"Alice\", \"John\"]\n",
    "\n",
    "    def invoke(self, input_value):\n",
    "        out = {\"tool_calls\": []}\n",
    "        tool_call = {\"name\": \"unknown\"}\n",
    "\n",
    "        # parse out the function name\n",
    "        if \"delete\" in input_value:\n",
    "            tool_call[\"name\"] = \"delete_account\"\n",
    "        if \"create\" in input_value:\n",
    "            tool_call[\"name\"] = \"create_account\"\n",
    "\n",
    "        # parse out the function parameter\n",
    "        tool_call[\"parameters\"] = {}\n",
    "        for username in self.usernames:\n",
    "            if username in input_value:\n",
    "                tool_call[\"parameters\"][\"username\"] = username\n",
    "                break\n",
    "\n",
    "        # package the tool call and return\n",
    "        out[\"tool_calls\"].append(tool_call)\n",
    "        return ModelInvocation(\n",
    "            model_prediction=out,\n",
    "            model_input=input_value,\n",
    "        )\n",
    "\n",
    "# Register the model to use in the test run\n",
    "mut_name=\"Function Call Demo Model\"\n",
    "model_under_test = okareo.register_model(\n",
    "    name=mut_name,\n",
    "    model=[FunctionCallModel(name=FunctionCallModel.__name__)],\n",
    "    update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a [Generation evaluation](https://docs.okareo.ai/docs/guides/generation_overview) on the custom model. \n",
    "\n",
    "We use predefined [checks in Okareo](https://docs.okareo.ai/docs/getting-started/concepts/checks), and the selected checks reflect the [Evaluation Metrics](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html#metrics) used in the Berkeley Tool Calling Leaderboard. These checks include:\n",
    "\n",
    "- \"Is Function Correct\"\n",
    "- \"Are Required Parameters Present\"\n",
    "- \"Are All Parameters Expected\"\n",
    "- \"Do Parameter Values Match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation that uses the scenario, check, and model\n",
    "from okareo_api_client.models.test_run_type import TestRunType\n",
    "\n",
    "eval_name = f\"Function Call Demo Evaluation\"\n",
    "evaluation = model_under_test.run_test(\n",
    "    name=eval_name,\n",
    "    scenario=tool_scenario.scenario_id,\n",
    "    test_run_type=TestRunType.NL_GENERATION,\n",
    "    checks=[\n",
    "        \"is_function_correct\",\n",
    "        \"are_required_params_present\",\n",
    "        \"are_all_params_expected\",\n",
    "        \"do_param_values_match\",\n",
    "    ],\n",
    ")\n",
    "print(f\"See results in Okareo: {evaluation.app_link}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
