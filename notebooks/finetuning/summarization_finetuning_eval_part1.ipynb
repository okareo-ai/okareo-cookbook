{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Finetuning of Summarization Models in Okareo\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/classification_finetuning_eval_part1.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## ðŸŽ¯ Goals\n",
    "\n",
    "After using this notebook, you will be able to:\n",
    "- Finetune an open source LLM for summarization\n",
    "- Evaluate the finetuned model in Okareo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement: Summarization\n",
    "\n",
    "Suppose we are developing a meeting transcript summarization application. To do this, we will prompt an LLM to take a meeting transcript (several hundred to a thousand words) and to generate a concise summary.\n",
    "\n",
    "We will start by evaluating a zero-shot summarization LLM (GPT-4o-mini). We will then attempt to improve the latency and performance by fine-tuning an open source model for our summarization task.\n",
    "\n",
    "This notebook focuses on finetuning an open source LLM, [Phi-3.5-mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct), for summarization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the Data as a Scenario in Okareo\n",
    "\n",
    "First, we setup our Okareo client. You will need API token from [https://app.okareo.com/](https://app.okareo.com/). (Note: You will need to sign up and generate an API token.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Okareo client\n",
    "\n",
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = \"<YOUR_OKAREO_API_KEY>\"\n",
    "okareo = Okareo(OKAREO_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all meeting summaries\n",
    "import pandas as pd\n",
    "meeting_transcripts = \"../../demos/.okareo/flows/meetings_long.jsonl\"\n",
    "json_df = pd.read_json(meeting_transcripts, lines=True)\n",
    "\n",
    "json_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick transcripts that are shorter than 2048\n",
    "\n",
    "N = 2048\n",
    "sampled_df = json_df[json_df['source'].apply(lambda x: len(x) < 2048)].reset_index(drop=True)\n",
    "print(f\"# summaries shorter than {N} characters: {sampled_df.shape[0]}\")\n",
    "sampled_df = sampled_df.loc[:,['summary', 'source']]\n",
    "sampled_df = sampled_df.rename(columns={'source': 'input', 'summary': 'result'})\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max length (in both characters/words) of the reference summaries\n",
    "n_words_max = sampled_df['result'].apply(lambda x: len(x.split(\" \"))).max()\n",
    "n_chars_max = sampled_df['result'].apply(lambda x: len(x)).max()\n",
    "n_words_mean = sampled_df['result'].apply(lambda x: len(x.split(\" \"))).mean()\n",
    "n_chars_mean = sampled_df['result'].apply(lambda x: len(x)).mean()\n",
    "n_words_median = sampled_df['result'].apply(lambda x: len(x.split(\" \"))).median()\n",
    "n_chars_median = sampled_df['result'].apply(lambda x: len(x)).median()\n",
    "print(f\"Max words: {n_words_max} | Max characters: {n_chars_max}\")\n",
    "print(f\"Mean words: {n_words_mean} | Mean characters: {n_chars_mean}\")\n",
    "print(f\"Median words: {n_words_median} | Median characters: {n_chars_median}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can get an LLM to summarize the meeting transcripts in 350 words or fewer (i.e., around the median character length in the transcript dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sampled_df['input'], sampled_df['result'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo_api_client.models.scenario_set_create import ScenarioSetCreate\n",
    "from okareo_api_client.models.seed_data import SeedData\n",
    "\n",
    "# Create a scenario set of the WebBizz documents\n",
    "\n",
    "split_names = [\"train\", \"test\"]\n",
    "input_splits = [X_train, X_test]\n",
    "result_splits = [y_train, y_test]\n",
    "split_ids = {}\n",
    "\n",
    "for split_name, input_split, result_split in zip(split_names, input_splits, result_splits):\n",
    "    seed_data = []\n",
    "    for article, summary in zip(input_split, result_split):\n",
    "        seed_data.append(SeedData(input_=article, result=summary))\n",
    "\n",
    "    summary_scenario = okareo.create_scenario_set(\n",
    "        ScenarioSetCreate(\n",
    "            name=f\"Meeting Summaries ({split_name})\", seed_data=seed_data\n",
    "        )\n",
    "    )\n",
    "    split_ids[split_name] = summary_scenario.scenario_id\n",
    "    print(summary_scenario.app_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT_ID = \"0081b0e2-225a-4d2b-b73f-8af46561da6c\"\n",
    "TEST_SPLIT_ID = \"bf732090-785c-42e4-9eb9-25ccf7d51794\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in addition to pre-defined Okareo checks, we will use a few custom CodeBasedChecks to evaluate the model\n",
    "\n",
    "from checks.character_count import Check\n",
    "\n",
    "okareo.create_or_update_check(\n",
    "    name=\"character_count\",\n",
    "    description=\"The number of characters in the model_output.\",\n",
    "    check=Check(),\n",
    ")\n",
    "\n",
    "from checks.word_count import Check\n",
    "\n",
    "okareo.create_or_update_check(\n",
    "    name=\"word_count\",\n",
    "    description=\"The number of words in the model_output.\",\n",
    "    check=Check(),\n",
    ")\n",
    "\n",
    "from checks.is_character_count_under_350 import Check\n",
    "\n",
    "okareo.create_or_update_check(\n",
    "    name=\"under_350_characters\",\n",
    "    description=\"Whether or not the number of characters in the model_output is under 350.\",\n",
    "    check=Check(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test split. We will use the train split later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from okareo.model_under_test import OpenAIModel\n",
    "from okareo_api_client.models import TestRunType\n",
    "\n",
    "OPENAI_API_KEY=\"<YOUR_OPENAI_API_KEY>\"\n",
    "\n",
    "with open('prompts/zero_shot_summarization.txt', \"r\") as f:\n",
    "    zero_shot_prompt = f.read()\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"Article: {scenario_input}\"\n",
    "\n",
    "model_name = \"GPT-4o-mini (Zero-Shot Summarization)\"\n",
    "mut = okareo.register_model(\n",
    "    name=model_name,\n",
    "    model=OpenAIModel(\n",
    "        model_id=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        system_prompt_template=zero_shot_prompt,\n",
    "        user_prompt_template=USER_PROMPT_TEMPLATE,\n",
    "    ),\n",
    "    update=True,\n",
    ")\n",
    "\n",
    "test_run_name = f\"Summarization Run (test split)\"\n",
    "run_resp = mut.run_test(\n",
    "    name=test_run_name,\n",
    "    scenario=TEST_SPLIT_ID,#split_ids['test'],\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    test_run_type=TestRunType.NL_GENERATION,\n",
    "    checks=[\n",
    "        \"latency\",\n",
    "        \"fluency_summary\",\n",
    "        \"character_count\",\n",
    "        \"word_count\",\n",
    "        \"under_350_characters\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_resp.app_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune Phi-3.5-mini-instruct for Summarization\n",
    "\n",
    "With our goal of reducing the length of summaries in mind, let's fine-tune an open source LLM for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data points\n",
    "sdp = okareo.get_scenario_data_points(TRAIN_SPLIT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_data = [{'input': dp.input_, 'result': dp.result} for dp in sdp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Phi-3 for finetuning\n",
    "\n",
    "Now we set up a finetuning run on [Phi-3.5-mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) using the finetuning instruction scenario.\n",
    "\n",
    "Setup is mostly boilerplate from [here](https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/sample_finetune.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import datasets\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "###################\n",
    "# Hyper-parameters\n",
    "###################\n",
    "training_config = {\n",
    "    \"bf16\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"learning_rate\": 5.0e-05,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 20,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./finetuned_phi3\", # checkpoint directory\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "    }\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Param efficient finetuning config\n",
    "peft_config = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    # \"target_modules\": \"all-linear\",\n",
    "    \"modules_to_save\": None,\n",
    "}\n",
    "train_conf = TrainingArguments(**training_config)\n",
    "peft_conf = LoraConfig(**peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "###############\n",
    "# Setup logging\n",
    "###############\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "log_level = train_conf.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process a small summary\n",
    "logger.warning(\n",
    "    f\"Process rank: {train_conf.local_rank}, device: {train_conf.device}, n_gpu: {train_conf.n_gpu}\"\n",
    "    + f\" distributed training: {bool(train_conf.local_rank != -1)}, 16-bits training: {train_conf.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {train_conf}\")\n",
    "logger.info(f\"PEFT parameters {peft_conf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Model Loading\n",
    "################\n",
    "\n",
    "checkpoint_path = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=\"flash_attention_2\",  # loading the model with flash-attenstion support\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 3072\n",
    "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Data Processing\n",
    "##################\n",
    "\n",
    "with open('prompts/finetune_summarization.txt', \"r\") as f:\n",
    "    SHORT_SYSTEM_PROMPT_TEMPLATE = f.read()\n",
    "\n",
    "def apply_chat_template(\n",
    "    example,\n",
    "    tokenizer,\n",
    "):\n",
    "    messages = example[\"messages\"]\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "message_formatted_data = []\n",
    "for data in finetuning_data:\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': f\"{SHORT_SYSTEM_PROMPT_TEMPLATE}\\n\\nArticle: {data['input']}\"},\n",
    "        {'role': 'assistant', 'content': data['result']},\n",
    "    ]\n",
    "    message_formatted_data.append({'messages': messages})\n",
    "\n",
    "dataset = Dataset.from_list(message_formatted_data)\n",
    "\n",
    "column_names = list(dataset.features)\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=10,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Applying chat template to train_sft\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Training\n",
    "###########\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_conf,\n",
    "    peft_config=peft_conf,\n",
    "    train_dataset=processed_dataset,\n",
    "    # eval_dataset=processed_test_dataset,\n",
    "    max_seq_length=3072,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True\n",
    ")\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############\n",
    "# # Save model\n",
    "# ############\n",
    "trainer.save_model(train_conf.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go on to Part 2 to run the evaluation of the fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TEST_SPLIT_ID={TEST_SPLIT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
