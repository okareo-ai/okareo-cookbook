{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Finetuning of Summarization Models in Okareo\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/classification_finetuning_eval_part1.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This notebook is Part 2 of our demo notebooks for evaluating fine-tuned summarization models in Okareo. If you have not already, then please run Part 1 to evaluate the baseline zero-shot model and to get the weights for fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get this ID from the last cell of Part 1\n",
    "TEST_SCENARIO_ID = \"bf732090-785c-42e4-9eb9-25ccf7d51794\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the Data as a Scenario in Okareo\n",
    "\n",
    "First, we setup our Okareo client. You will need API token from [https://app.okareo.com/](https://app.okareo.com/). (Note: You will need to register first.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Okareo client\n",
    "\n",
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = \"<YOUR_OKAREO_API_KEY>\"\n",
    "okareo = Okareo(OKAREO_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdp = okareo.get_scenario_data_points(TEST_SCENARIO_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Phi-3 for finetuning\n",
    "\n",
    "Now we set up a finetuning run on [Phi-3.5-mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) using the finetuning instruction scenario.\n",
    "\n",
    "Setup is mostly boilerplate from [here](https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/sample_finetune.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "###################\n",
    "# Hyper-parameters\n",
    "###################\n",
    "training_config = {\n",
    "    \"bf16\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"learning_rate\": 5.0e-05,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 20,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./finetuned_phi3\", # checkpoint directory\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "    }\n",
    "\n",
    "peft_config = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    # \"target_modules\": \"all-linear\",\n",
    "    \"modules_to_save\": None,\n",
    "}\n",
    "train_conf = TrainingArguments(**training_config)\n",
    "peft_conf = LoraConfig(**peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "###############\n",
    "# Setup logging\n",
    "###############\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "log_level = train_conf.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process a small summary\n",
    "logger.warning(\n",
    "    f\"Process rank: {train_conf.local_rank}, device: {train_conf.device}, n_gpu: {train_conf.n_gpu}\"\n",
    "    + f\" distributed training: {bool(train_conf.local_rank != -1)}, 16-bits training: {train_conf.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {train_conf}\")\n",
    "logger.info(f\"PEFT parameters {peft_conf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Get finetuned model/tokenizer\n",
    "########\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",  # loading the model with flash-attenstion support\n",
    "    device_map=\"auto\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    train_conf.output_dir,\n",
    "    **model_kwargs,\n",
    ")\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(train_conf.output_dir)\n",
    "ft_tokenizer.model_max_length = 3072\n",
    "ft_tokenizer.pad_token = ft_tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "ft_tokenizer.pad_token_id = ft_tokenizer.convert_tokens_to_ids(ft_tokenizer.pad_token)\n",
    "ft_tokenizer.padding_side = 'left' # required for correct generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo.model_under_test import CustomBatchModel, ModelInvocation\n",
    "\n",
    "with open('prompts/finetune_summarization.txt', \"r\") as f:\n",
    "    SHORT_SYSTEM_PROMPT_TEMPLATE = f.read()\n",
    "\n",
    "class Phi3SummaryModel(CustomBatchModel):\n",
    "    def __init__(self, name, batch_size):\n",
    "        super().__init__(name, batch_size)\n",
    "        # self.len_end_token = len(\"<|end|>\")\n",
    "        self.tokenizer = ft_tokenizer\n",
    "        self.model = ft_model\n",
    "\n",
    "    def invoke_batch(self, input_batch):\n",
    "        # unpack the input_values, ids from the batch\n",
    "        input_values = [input_dict['input_value'] for input_dict in input_batch]\n",
    "        scenario_ids = [input_dict['id'] for input_dict in input_batch]\n",
    "\n",
    "        prompts = []\n",
    "        prompt_messages = []\n",
    "        for input_value in input_values:\n",
    "            messages = [\n",
    "                {'role': 'user', 'content': f\"{SHORT_SYSTEM_PROMPT_TEMPLATE}\\n\\nArticle: {input_value}\"},\n",
    "            ]\n",
    "            prompt_messages.append(messages[0]['content'])\n",
    "            prompt = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_tokens = self.tokenizer(\n",
    "                prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                # truncation=True,\n",
    "            )\n",
    "            input_ids = input_tokens.input_ids.cuda()\n",
    "            attention_mask = input_tokens.attention_mask.cuda()\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=3072,\n",
    "                do_sample=False,\n",
    "            )\n",
    "            decoded_batch = ft_tokenizer.batch_decode(\n",
    "                outputs.detach().cpu().numpy(),\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "        del input_ids, attention_mask, outputs, input_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        invocations = []\n",
    "        for scenario_id, decoded, prompt, input_value, message in zip(scenario_ids, decoded_batch, prompts, input_values, prompt_messages):\n",
    "            pred = decoded[len(message):].strip() # only use generation past the instruction prompt\n",
    "            invocations.append({\n",
    "                'id': scenario_id,\n",
    "                'model_invocation': ModelInvocation(\n",
    "                    model_prediction=pred,\n",
    "                    model_input=input_value,\n",
    "                    model_output_metadata=decoded,\n",
    "                )\n",
    "            })\n",
    "        return invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from okareo_api_client.models.test_run_type import TestRunType\n",
    "\n",
    "OPENAI_API_KEY=\"<YOUR_OPENAI_API_KEY>\"\n",
    "\n",
    "batch_size = 64\n",
    "mut_name = \"Phi-3.5-mini-instruct (Fine-tuned Summarization)\"\n",
    "\n",
    "print(f'--- evaluation on test split ---')\n",
    "print(f'batch_size | eval_time (s) | app_link')\n",
    "# Register the model to use in the test run\n",
    "start_time = time()\n",
    "model_under_test = okareo.register_model(\n",
    "    name=mut_name,\n",
    "    model=[\n",
    "        Phi3SummaryModel(\n",
    "            name=\"Phi3SummaryModel(CustomBatchModel)\",\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    ],\n",
    "    update=True\n",
    ")\n",
    "\n",
    "eval_name = f\"Summarization Run (test split)\"\n",
    "evaluation = model_under_test.run_test(\n",
    "    name=eval_name,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    scenario=TEST_SCENARIO_ID,\n",
    "    test_run_type=TestRunType.NL_GENERATION,\n",
    "    checks=[\n",
    "        \"latency\",\n",
    "        \"fluency_summary\",\n",
    "        \"character_count\",\n",
    "        \"word_count\",\n",
    "        \"under_350_characters\",\n",
    "    ],\n",
    ")\n",
    "eval_time = time() - start_time\n",
    "print(f\"{batch_size} | {eval_time:3.2f} | {evaluation.app_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
