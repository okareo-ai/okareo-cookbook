{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Fine-tuned Retrieval Models in Okareo\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/retrieval_embedding_finetuning_eval.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## ðŸŽ¯ Goals\n",
    "\n",
    "After using this notebook, you will be able to:\n",
    "- Evaluate a pre-trained embedding model in Okareo\n",
    "- Filter the results of the retrieval evaluation\n",
    "- Generate fine-tuning data based on the filtered results\n",
    "- Fine-tune the model with the generated data\n",
    "- Compare the performance of the embedding models pre/post fine-tuning in Okareo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement: Retrieval Model\n",
    "\n",
    "Suppose we are developing a RAG system that answers user questions about an online retailer called WebBizz.\n",
    "\n",
    "This notebook focuses on finetuning an open source embedding model, [all-MiniLLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2), for the Context Retrieval component of the RAG pipeline.\n",
    "\n",
    "The purpose of Context Retrieval is to fetch relevant documents/chunks to build the context for a downstream generative model. The better performance we achieve on Retrieval, the higher quality the RAG's final output will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the embedding model + WebBizz retrieval dataset\n",
    "\n",
    "We start by loading a pre-trained `sentence_transformer` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mason/miniconda3/envs/transformers/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a model to train/finetune\n",
    "huggingface_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(huggingface_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load our WebBizz data, including:\n",
    "- Corpus: {\"id\": string, \"doc\": string}\n",
    "    - Corpus of WebBizz articles\n",
    "- Queries: {\"id\": string, \"query\": string}\n",
    "    - User queries\n",
    "- Relevancy data: {\"qid\": string, \"dids\": List[string]}\n",
    "    - Maps query ID -> relevant doc IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wb_corpus = load_dataset(\"json\", data_files=\"data/webbizz_corpus.jsonl\")\n",
    "wb_queries = load_dataset(\"json\", data_files=\"data/webbizz_queries.jsonl\")\n",
    "wb_relevant_docs_data = load_dataset(\"json\", data_files=\"data/webbizz_qrels.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datasets to dictionaries\n",
    "corpus = dict(zip(wb_corpus['train'][\"id\"], wb_corpus['train'][\"doc\"]))  # Our corpus (cid => document)\n",
    "queries = dict(zip(wb_queries['train'][\"id\"], wb_queries['train'][\"query\"]))  # Our queries (qid => question)\n",
    "relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
    "for qid, corpus_ids in zip(wb_relevant_docs_data['train'][\"query-id\"], wb_relevant_docs_data['train'][\"corpus-id\"]):\n",
    "    qid = str(qid)\n",
    "    corpus_ids = str(corpus_ids)\n",
    "    if qid not in relevant_docs:\n",
    "        relevant_docs[qid] = set()\n",
    "    relevant_docs[qid].add(corpus_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split relevant docs into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_qids, test_qids = train_test_split(list(relevant_docs.keys()), test_size=0.5, random_state=42)\n",
    "relevant_docs_train = {qid: relevant_docs[qid] for qid in train_qids}\n",
    "relevant_docs_test = {qid: relevant_docs[qid] for qid in test_qids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the pre-trained model in Okareo\n",
    "\n",
    "Here, we perform a retrieval evaluation in Okareo by:\n",
    "1. Uploading the WebBizz retrieval data as a scenario in Okareo\n",
    "2. Defining a CustomModel for retrieval using ChromaDB to store our model's embeddings\n",
    "3. Run a retrieval evaluation on our CustomModel with the WebBizz scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = os.environ[\"OKAREO_API_KEY\"]\n",
    "okareo = Okareo(OKAREO_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload user queries as scenario to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The name for a scenario must be unique. The scenario name WebBizz Retrieval - Queries (train) is already in use, and has been returned from this call.\n",
      "train: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/scenario/0564350b-1363-493f-989f-d6d09c4d6ce6\n",
      "Warning: The name for a scenario must be unique. The scenario name WebBizz Retrieval - Queries (test) is already in use, and has been returned from this call.\n",
      "test: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/scenario/a98bbd15-c2c5-44aa-91b1-8149bec11de2\n"
     ]
    }
   ],
   "source": [
    "from okareo_api_client.models import SeedData\n",
    "from okareo_api_client.models.scenario_set_create import ScenarioSetCreate\n",
    "\n",
    "# use queries + qrels to upload eval scenario\n",
    "scenarios = {}\n",
    "for r_docs, split_name in [(relevant_docs_train, \"train\"), (relevant_docs_test, \"test\")]:\n",
    "    scenario_points = [\n",
    "        SeedData(\n",
    "            input_=queries[qid],\n",
    "            result=list(dids),\n",
    "        ) for qid, dids in r_docs.items()\n",
    "    ]\n",
    "\n",
    "    # create the scenario set for evaluation\n",
    "    create_request = ScenarioSetCreate(\n",
    "        name=f\"WebBizz Retrieval - Queries ({split_name})\",\n",
    "        seed_data = scenario_points,\n",
    "    )\n",
    "\n",
    "    seed_scenario = okareo.create_scenario_set(create_request)\n",
    "    scenarios[split_name] = seed_scenario\n",
    "    print(f\"{split_name}: {seed_scenario.app_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload a custom retrieval model using a Chroma collection of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.create_collection(name=\"retrieval_test\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "collection.add(\n",
    "    documents=list(corpus.values()),\n",
    "    ids=list(corpus.keys()),\n",
    "    embeddings=model.encode(list(corpus.values())),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A funtion to convert the query results from our ChromaDB collection into a list of dictionaries with the document ID, score, metadata, and label\n",
    "def query_results_to_score(results):\n",
    "    parsed_ids_with_scores = []\n",
    "    for i in range(0, len(results['distances'][0])):\n",
    "        # Create a score based on cosine similarity\n",
    "        score = (2 - results['distances'][0][i]) / 2\n",
    "        parsed_ids_with_scores.append(\n",
    "            {\n",
    "                \"id\": results['ids'][0][i],\n",
    "                \"score\": score,\n",
    "                \"metadata\": {\"article\": corpus[results['ids'][0][i]]},\n",
    "                \"label\": f\"WebBizz Article w/ ID: {results['ids'][0][i]}\"\n",
    "            }\n",
    "        )\n",
    "    return parsed_ids_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo.model_under_test import CustomModel, ModelInvocation\n",
    "\n",
    "mut_name = f\"Retrieval Model - all-MiniLM-L6-v2\"\n",
    "\n",
    "class RetrievalModel(CustomModel):\n",
    "    def invoke(self, input: str):\n",
    "        embeddings = model.encode([input])\n",
    "        results = collection.query(\n",
    "            query_embeddings=embeddings,\n",
    "            n_results=5\n",
    "        )\n",
    "        # return a tuple of (parsed_ids_with_scores, overall model response context)\n",
    "        return ModelInvocation(\n",
    "            model_prediction=query_results_to_score(results),\n",
    "            model_input=input,\n",
    "            model_output_metadata={'model_data': input}\n",
    "        )\n",
    "\n",
    "# Register the model to use in the test run\n",
    "model_under_test = okareo.register_model(\n",
    "    name=mut_name,\n",
    "    model=[RetrievalModel(name=RetrievalModel.__name__)],\n",
    "    update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See train eval results in Okareo: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/bf952652-d692-42e7-bef4-eb7cd421b14b\n",
      "See test eval results in Okareo: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/a6a90510-210f-46a0-afdd-da42b69ec9ae\n"
     ]
    }
   ],
   "source": [
    "from okareo_api_client.models import TestRunType\n",
    "\n",
    "# Import the datetime module for timestamping\n",
    "from datetime import datetime\n",
    "\n",
    "# Define thresholds for the evaluation metrics\n",
    "at_k_intervals = [1, 2, 3, 4, 5] \n",
    "metrics_kwargs={\n",
    "    \"accuracy_at_k\": at_k_intervals ,\n",
    "    \"precision_recall_at_k\": at_k_intervals ,\n",
    "    \"ndcg_at_k\": at_k_intervals,\n",
    "    \"mrr_at_k\": at_k_intervals,\n",
    "    \"map_at_k\": at_k_intervals,\n",
    "}\n",
    "\n",
    "# Perform a test run using the uploaded scenario set\n",
    "test_runs = {}\n",
    "for split_name, seed_scenario in scenarios.items():\n",
    "    test_run_item = model_under_test.run_test(\n",
    "        scenario=seed_scenario, # use the scenario from the scenario set uploaded earlier\n",
    "        name=f\"Retrieval ({split_name}) - {datetime.now().strftime('%m-%d %H:%M:%S')}\", # add a timestamp to the test run name\n",
    "        test_run_type=TestRunType.INFORMATION_RETRIEVAL, # specify that we are running an information retrieval test\n",
    "        calculate_metrics=True,\n",
    "        # Define the evaluation metrics to calculate\n",
    "        metrics_kwargs=metrics_kwargs\n",
    "    )\n",
    "\n",
    "    # Generate a link back to Okareo for evaluation visualization\n",
    "    model_results = test_run_item.model_metrics.to_dict()\n",
    "    test_runs[split_name] = model_results\n",
    "    app_link = test_run_item.app_link\n",
    "    print(f\"See {split_name} eval results in Okareo: {app_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand the finetuning set with Failure Rows\n",
    "\n",
    "To improve our finetuned model, we need a fine-tuning set that is similar to our WebBizz data. To do this, we extract rows from our retrieval evaluation based on some failure criteria, and we generate new queries based on these failed rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 10 queries have recall@5 <= 0.5\n"
     ]
    }
   ],
   "source": [
    "# filter IDs based on failure criteria\n",
    "K = \"5\"\n",
    "filter_thresh = 0.5\n",
    "filter_metric = \"recall\"\n",
    "\n",
    "# get failure rows from train split eval\n",
    "failed_ids = []\n",
    "for id, metrics in test_runs['train']['row_level_metrics'].items():\n",
    "    if metrics[K][filter_metric] <= filter_thresh:\n",
    "        failed_ids.append(id)\n",
    "print(f\"-> {len(failed_ids)} queries have {filter_metric}@{K} <= {filter_thresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/scenario/3f99ab26-b187-499c-b501-6c6dc84e6e82'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use queries + qrels to upload failure scenario\n",
    "sdp = okareo.get_scenario_data_points(scenarios['train'].scenario_id)\n",
    "\n",
    "scenario_points = []\n",
    "query_to_id = {v: k for k, v in queries.items()}\n",
    "for dp in sdp:\n",
    "    # get the qid for failed okareo scenario IDs\n",
    "    if dp.id in failed_ids:\n",
    "        qid = query_to_id[dp.input_]\n",
    "        dids = relevant_docs[qid]\n",
    "        scenario_points.append(\n",
    "            SeedData(\n",
    "                input_=queries[qid],\n",
    "                result=list(dids),\n",
    "            )\n",
    "        )\n",
    "\n",
    "# create the scenario set for evaluation\n",
    "create_request = ScenarioSetCreate(\n",
    "    name=\"WebBizz Retrieval - Queries (train; failure Rows)\",\n",
    "    seed_data = scenario_points,\n",
    ")\n",
    "\n",
    "failure_scenario = okareo.create_scenario_set(create_request)\n",
    "failure_scenario.app_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new queries based on the failure rows\n",
    "from okareo_api_client.models.scenario_set_generate import ScenarioSetGenerate\n",
    "from okareo_api_client.models.generation_tone import GenerationTone\n",
    "from okareo_api_client.models.scenario_type import ScenarioType\n",
    "\n",
    "generate_request = ScenarioSetGenerate(\n",
    "    source_scenario_id=failure_scenario.scenario_id,\n",
    "    name=\"WebBizz Retrieval - Queries (train; rephrased failure rows)\",\n",
    "    number_examples=3,\n",
    "    generation_type=ScenarioType.REPHRASE_INVARIANT,\n",
    "    generation_tone=GenerationTone.INFORMAL,\n",
    ")\n",
    "\n",
    "rephrased_scenario = okareo.generate_scenario_set(generate_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n"
     ]
    }
   ],
   "source": [
    "# format the scenario data points properly for the huggingface trainer\n",
    "# this requires that we pivot each {'query': ..., 'answers': [1, ..., N]}\n",
    "# to {'query': ..., 'answer': 1}, ..., {'query': ..., 'answer': N}\n",
    "rephrased_sdp = okareo.get_scenario_data_points(rephrased_scenario.scenario_id)\n",
    "\n",
    "# augment the train data with the rephrased failure rows\n",
    "# use the augmented train set for finetuning\n",
    "finetuning_embedding_data = []\n",
    "for dp in sdp + rephrased_sdp:\n",
    "    for did in dp.result:\n",
    "        finetuning_embedding_data.append({'query': dp.input_, 'answer': corpus[did]})\n",
    "\n",
    "print(len(finetuning_embedding_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = f\"./data/webbizz_finetuning_embedding_data.jsonl\"\n",
    "\n",
    "# write the finetuning data to a jsonl file\n",
    "with open(file_path, \"w\") as file:\n",
    "    for row in finetuning_embedding_data:\n",
    "        file.write(json.dumps(row) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the Model on the Augmented Train Split\n",
    "\n",
    "Use the train split and the rephrased failures to repeat fine-tune the embedding model, and then compare the retrieval performance of the fine-tuned model to the pretrained model in Okareo.\n",
    "\n",
    "For more details on fine-tuning embedding models, see this [huggingface blog on training sentence transformers](https://huggingface.co/blog/train-sentence-transformers#local-data-that-requires-pre-processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953fccc33d8a46429906735e82001e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "\n",
    "# Load a model to train/finetune\n",
    "huggingface_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(huggingface_model_name)\n",
    "\n",
    "# Initialize the MultipleNegativesRankingLoss\n",
    "# This loss requires pairs of queries and related document chunks\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Load an example training dataset that works with our loss function:\n",
    "dataset = load_dataset(\"json\", data_files=\"./data/webbizz_finetuning_embedding_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=f\"models/{huggingface_model_name}-webbizz/1epoch\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32, # smaller batch sizes for our smaller dataset\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,  # Set to False if your GPU can't handle FP16\n",
    "    bf16=False,  # Set to True if your GPU supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # Losses using \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda9c8a143d14003a2aead6579d92661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3.4635, 'train_samples_per_second': 53.414, 'train_steps_per_second': 1.732, 'train_loss': 2.257785161336263, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=2.257785161336263, metrics={'train_runtime': 3.4635, 'train_samples_per_second': 53.414, 'train_steps_per_second': 1.732, 'total_flos': 0.0, 'train_loss': 2.257785161336263, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainer\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the fine-tuned model\n",
    "\n",
    "Create a new ChromaDB collection with the updated document embeddings to use with our new fine-tuned CustomModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_collection = chroma_client.create_collection(name=\"retrieval_finetune_test\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "finetuned_collection.add(\n",
    "    documents=list(corpus.values()),\n",
    "    ids=list(corpus.keys()),\n",
    "    embeddings=model.encode(list(corpus.values())),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_name = f\"Finetuned Retrieval Model - all-MiniLM-L6-v2\"\n",
    "\n",
    "class FinetunedRetrievalModel(CustomModel):\n",
    "    def invoke(self, input: str):\n",
    "        embeddings = model.encode([input])\n",
    "        results = finetuned_collection.query(\n",
    "            query_embeddings=embeddings,\n",
    "            n_results=5\n",
    "        )\n",
    "        # return a tuple of (parsed_ids_with_scores, overall model response context)\n",
    "        return ModelInvocation(\n",
    "            model_prediction=query_results_to_score(results),\n",
    "            model_input=input,\n",
    "            model_output_metadata={'model_data': input}\n",
    "        )\n",
    "\n",
    "# Register the model to use in the test run\n",
    "model_under_test = okareo.register_model(\n",
    "    name=mut_name,\n",
    "    model=[FinetunedRetrievalModel(name=FinetunedRetrievalModel.__name__)],\n",
    "    update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See train eval results in Okareo: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/8c43dc0c-3c9c-437c-a061-936cc6f49332\n",
      "See test eval results in Okareo: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/297a3688-bf65-487f-b39d-f28f48bfaf28\n"
     ]
    }
   ],
   "source": [
    "# Perform a test run using the uploaded scenario set\n",
    "finetuned_test_runs = {}\n",
    "for split_name, seed_scenario in scenarios.items():\n",
    "    test_run_item = model_under_test.run_test(\n",
    "        scenario=seed_scenario, # use the scenario from the scenario set uploaded earlier\n",
    "        name=f\"Retrieval ({split_name}) - {datetime.now().strftime('%m-%d %H:%M:%S')}\", # add a timestamp to the test run name\n",
    "        test_run_type=TestRunType.INFORMATION_RETRIEVAL, # specify that we are running an information retrieval test\n",
    "        calculate_metrics=True,\n",
    "        # Define the evaluation metrics to calculate\n",
    "        metrics_kwargs=metrics_kwargs\n",
    "    )\n",
    "\n",
    "    # Generate a link back to Okareo for evaluation visualization\n",
    "    finetuned_model_results = test_run_item.model_metrics.to_dict()\n",
    "    finetuned_test_runs[split_name] = finetuned_model_results\n",
    "    app_link = test_run_item.app_link\n",
    "    print(f\"See {split_name} eval results in Okareo: {app_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Fine-tuning | Post Fine-tuning | Difference\n",
      "------ train ------\n",
      "------ Accuracy@k ------\n",
      "K=1: 0.833 | 1.000 | +0.167\n",
      "K=2: 0.944 | 1.000 | +0.056\n",
      "K=3: 0.972 | 1.000 | +0.028\n",
      "K=4: 1.000 | 1.000 | +0.000\n",
      "K=5: 1.000 | 1.000 | +0.000\n",
      "------ Precision@k ------\n",
      "K=1: 0.833 | 1.000 | +0.167\n",
      "K=2: 0.597 | 0.750 | +0.153\n",
      "K=3: 0.472 | 0.583 | +0.111\n",
      "K=4: 0.375 | 0.458 | +0.083\n",
      "K=5: 0.328 | 0.389 | +0.061\n",
      "------ Recall@k ------\n",
      "K=1: 0.515 | 0.580 | +0.066\n",
      "K=2: 0.645 | 0.767 | +0.122\n",
      "K=3: 0.731 | 0.845 | +0.114\n",
      "K=4: 0.756 | 0.869 | +0.114\n",
      "K=5: 0.807 | 0.911 | +0.104\n",
      "------ NDCG@k ------\n",
      "K=1: 0.833 | 1.000 | +0.167\n",
      "K=2: 0.769 | 0.925 | +0.156\n",
      "K=3: 0.766 | 0.906 | +0.139\n",
      "K=4: 0.774 | 0.909 | +0.135\n",
      "K=5: 0.797 | 0.924 | +0.127\n",
      "------ MRR@k ------\n",
      "K=1: 0.833 | 1.000 | +0.167\n",
      "K=2: 0.889 | 1.000 | +0.111\n",
      "K=3: 0.898 | 1.000 | +0.102\n",
      "K=4: 0.905 | 1.000 | +0.095\n",
      "K=5: 0.905 | 1.000 | +0.095\n",
      "------ MAP@k ------\n",
      "K=1: 0.833 | 1.000 | +0.167\n",
      "K=2: 0.722 | 0.903 | +0.181\n",
      "K=3: 0.705 | 0.872 | +0.167\n",
      "K=4: 0.707 | 0.872 | +0.165\n",
      "K=5: 0.725 | 0.884 | +0.159\n",
      "------ test ------\n",
      "------ Accuracy@k ------\n",
      "K=1: 0.875 | 1.000 | +0.125\n",
      "K=2: 0.958 | 1.000 | +0.042\n",
      "K=3: 0.958 | 1.000 | +0.042\n",
      "K=4: 0.958 | 1.000 | +0.042\n",
      "K=5: 0.958 | 1.000 | +0.042\n",
      "------ Precision@k ------\n",
      "K=1: 0.875 | 1.000 | +0.125\n",
      "K=2: 0.604 | 0.625 | +0.021\n",
      "K=3: 0.486 | 0.472 | -0.014\n",
      "K=4: 0.396 | 0.427 | +0.031\n",
      "K=5: 0.342 | 0.367 | +0.025\n",
      "------ Recall@k ------\n",
      "K=1: 0.455 | 0.528 | +0.073\n",
      "K=2: 0.611 | 0.618 | +0.008\n",
      "K=3: 0.680 | 0.667 | -0.013\n",
      "K=4: 0.736 | 0.764 | +0.028\n",
      "K=5: 0.771 | 0.816 | +0.045\n",
      "------ NDCG@k ------\n",
      "K=1: 0.875 | 1.000 | +0.125\n",
      "K=2: 0.740 | 0.790 | +0.050\n",
      "K=3: 0.748 | 0.769 | +0.021\n",
      "K=4: 0.758 | 0.809 | +0.050\n",
      "K=5: 0.771 | 0.827 | +0.057\n",
      "------ MRR@k ------\n",
      "K=1: 0.875 | 1.000 | +0.125\n",
      "K=2: 0.917 | 1.000 | +0.083\n",
      "K=3: 0.917 | 1.000 | +0.083\n",
      "K=4: 0.917 | 1.000 | +0.083\n",
      "K=5: 0.917 | 1.000 | +0.083\n",
      "------ MAP@k ------\n",
      "K=1: 0.875 | 1.000 | +0.125\n",
      "K=2: 0.677 | 0.729 | +0.052\n",
      "K=3: 0.676 | 0.688 | +0.012\n",
      "K=4: 0.681 | 0.727 | +0.046\n",
      "K=5: 0.689 | 0.744 | +0.054\n"
     ]
    }
   ],
   "source": [
    "# compare the results pre/post fine-tuning\n",
    "\n",
    "print(f\"Pre Fine-tuning | Post Fine-tuning | Difference\")\n",
    "for split_name in [\"train\", \"test\"]:\n",
    "    print(f\"------ {split_name} ------\")\n",
    "    model_results = test_runs[split_name]\n",
    "    finetuned_model_results = finetuned_test_runs[split_name]\n",
    "    for key in finetuned_model_results.keys():\n",
    "        if key == \"row_level_metrics\":\n",
    "            continue\n",
    "        print(f\"------ {key} ------\")\n",
    "        for K in at_k_intervals:\n",
    "            pre = model_results[key][str(K)]\n",
    "            post = finetuned_model_results[key][str(K)]\n",
    "            diff = post - pre\n",
    "            print(f\"K={K}: {pre:4.3f} | {post:4.3f} | {'+' if diff >= 0 else ''}{diff:4.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all retrieval metrics have improved across all K values!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
